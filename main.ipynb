{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from math import gcd\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação de dados\n",
    "def create_data(options):\n",
    "  data = []\n",
    "\n",
    "  if options == 1:\n",
    "    while len(data) < 1000000:\n",
    "      a = np.random.randint(1, 101)\n",
    "      b = np.random.randint(1, 101)\n",
    "      data.append([a, b, gcd(a, b)])\n",
    "\n",
    "  elif options == 2:\n",
    "    for i in range(1, 101):\n",
    "      for j in range(1, 101):\n",
    "        data.append([i, j, gcd(i, j)])\n",
    "    \n",
    "    random.shuffle(data)\n",
    "    \n",
    "    # Dividir os dados em três partes e Expande os dados \n",
    "    first_half_size = len(data) // 2 # 50% para treinamento\n",
    "    second_quarter_size = len(data) // 4 # 25% para validação e teste\n",
    "\n",
    "    train_data = data[ : first_half_size] * 100\n",
    "    validation_data = data[first_half_size : first_half_size + second_quarter_size] * 100\n",
    "    test_data = data[first_half_size + second_quarter_size : ] * 100\n",
    "\n",
    "    random.shuffle(train_data)\n",
    "    random.shuffle(validation_data)\n",
    "\n",
    "    data = train_data + validation_data + test_data\n",
    "\n",
    "  # Escreve todos os dados em um arquivo\n",
    "  with open(\"txt_files/new_data.txt\", \"w\") as f:\n",
    "    for item in data:\n",
    "      f.write(\"\\t\".join(map(str, item)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "  with open(file_path, \"r\") as f:\n",
    "    data = [line.strip().split(\"\\t\") for line in f.readlines()]\n",
    "\n",
    "  y_in = np.array([[int(d[0]), int(d[1])] for d in data])\n",
    "  mdc = np.array([int(d[2]) for d in data])\n",
    "\n",
    "  # Primeira divisão entre treinamento e validação\n",
    "  y_train, y_validation, mdc_train, mdc_validation = train_test_split(y_in, mdc, test_size=0.5, shuffle=False)\n",
    "  # Segunda divisão dentro do conjunto de validação, entre validação e teste\n",
    "  y_validation, y_test, mdc_validation, mdc_test = train_test_split(y_validation, mdc_validation, test_size=0.5, shuffle=False)\n",
    "\n",
    "  return y_train, y_validation, y_test, mdc_train, mdc_validation, mdc_test\n",
    "\n",
    "def check_data(check_array, array_1, array_2):\n",
    "  # Converter array_1 e array_2 para conjuntos\n",
    "  set_array_1 = set(map(tuple, array_1))\n",
    "  set_array_2 = set(map(tuple, array_2))\n",
    "\n",
    "  dados_repetidos = 0\n",
    "\n",
    "  # Verificar se os dados de check_array estão em set_array_1 ou set_array_2\n",
    "  for i in map(tuple, check_array):\n",
    "    if i in set_array_1 or i in set_array_2:\n",
    "      dados_repetidos += 1\n",
    "\n",
    "  print(f\"Total de dados repetidos: {dados_repetidos}\")\n",
    "\n",
    "def delete_duplicate_ordanate(array):\n",
    "  # Converte o array numpy para uma lista de tuplas e depois para um conjunto para remover duplicatas\n",
    "  unique_set = set(map(tuple, array))\n",
    "  # Converte o conjunto de volta para um array numpy\n",
    "  unique_array = np.array(list(unique_set))\n",
    "\n",
    "  # Organizada o array em ordem crescente\n",
    "  sorted_indices = np.lexsort((unique_array[:,1], unique_array[:,0]))\n",
    "  sorted_array = unique_array[sorted_indices]\n",
    "  \n",
    "  return np.array(sorted_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opção 1 cria dados repetidos / Opção 2 cria dados diferentes para cada conjunto\n",
    "# Total de dados: 1.000.000\n",
    "option = 2\n",
    "create_data(option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_validation, y_test, mdc_train, mdc_validation, mdc_test = load_data(\"txt_files/new_data.txt\")\n",
    "\n",
    "check_data(y_train, y_validation, y_test)\n",
    "check_data(y_validation, y_train, y_test)\n",
    "check_data(y_test, y_train, y_validation)\n",
    "\n",
    "print(len(y_train), len(y_validation), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rede Neural\n",
    "model = Sequential([\n",
    "  Embedding(input_dim=201, output_dim=64), # Camada de entrada\n",
    "  LSTM(64, return_sequences=True),\n",
    "  LSTM(64, return_sequences=True),\n",
    "  LSTM(64),\n",
    "  Dense(1) # Camada de saída\n",
    "])\n",
    "\n",
    "# Compilar o modelo\n",
    "model.compile(optimizer = \"adam\", loss= \"mean_squared_error\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Callback para parar o treinamento se a métrica monitorada não melhorar\n",
    "early_stopping = EarlyStopping(monitor=\"loss\", patience=10)\n",
    "\n",
    "#Treina o modelo\n",
    "history = model.fit(y_train, mdc_train, epochs=500, batch_size=512, verbose=1, callbacks=[early_stopping], validation_data=(y_validation, mdc_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# Salva modelo treinado\n",
    "# model.save(\"modelos_historico/NOME.keras\")\n",
    "\n",
    "# Salva o histórico\n",
    "# with open(\"modelos_historico/NOME.pkl\", \"wb\") as file:\n",
    "#   pickle.dump(history.history, file)\n",
    "\n",
    "# Carrega modelo treinado\n",
    "# model = load_model(\"modelos_historico/NOME.keras\")\n",
    "\n",
    "# Carregar o histórico\n",
    "# with open(\"modelos_historico/different_data.pkl\", \"rb\") as file:\n",
    "#   history = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar a figura e os subplots\n",
    "fig, axs = plt.subplots(1,2, figsize=(14, 6))\n",
    "\n",
    "# Plotar o gráfico da CUSTO em função da época\n",
    "axs[0].plot(history['loss'], label=\"Custo (Dados de treinamento)\")\n",
    "axs[0].plot(history['val_loss'], label=\"Custo (Dados de validação)\")\n",
    "axs[0].set_title('Custo em função da Época')\n",
    "axs[0].set_xlabel('Época')\n",
    "axs[0].set_ylabel('Custo')\n",
    "axs[0].legend(loc=\"upper right\")\n",
    "axs[0].grid(True)\n",
    "\n",
    "# Plotar o gráfico da PRECISÃO em função da época\n",
    "axs[1].plot(history[\"accuracy\"], label=\"Precisão (Dados de treinamento)\")\n",
    "axs[1].plot(history['val_accuracy'], label=\"Precisão (Dados de validação)\")\n",
    "axs[1].set_title('Precisão em função da Época')\n",
    "axs[1].set_xlabel('Época')\n",
    "axs[1].set_ylabel('Precisão')\n",
    "axs[1].legend(loc=\"lower right\")\n",
    "axs[1].grid(True)\n",
    "\n",
    "# Ajustar o layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Mostrar a figura\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_y_test = delete_duplicate_ordanate(y_test)\n",
    "new_mdc_test = []\n",
    "\n",
    "for i in new_y_test:\n",
    "  new_mdc_test.append( gcd(i[0], i[1]) )\n",
    "\n",
    "predictions = model.predict(new_y_test)\n",
    "rounded_predictions = np.round(predictions).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_value = 0\n",
    "wrong_predictions = []\n",
    "\n",
    "for i in range(len(new_y_test)):\n",
    "  if (rounded_predictions[i] == new_mdc_test[i]):\n",
    "    correct_value += 1\n",
    "  else:\n",
    "    wrong_predictions.append(f\"MDC de {[new_y_test[i][0], new_y_test[i][1]]} - Previsto: {rounded_predictions[i][0], predictions[i][0]} - Real: {new_mdc_test[i]}\")\n",
    "\n",
    "print(f\"Dados (teste): {len(new_y_test)}\")\n",
    "print(f\"Acertos: {correct_value}\")\n",
    "print(f\"Erros: {len(wrong_predictions)}\")\n",
    "\n",
    "print(f\"Precisão: {((correct_value/len(new_y_test)) * 100):.2f}%\")\n",
    "\n",
    "# with open(\"txt_files/previsões_erradas_dados_diferentes.txt\", \"w\") as f:\n",
    "#   for i in range(len(wrong_predictions)):\n",
    "#     f.write(wrong_predictions[i] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Salva algumas predições erradas da rede neural\n",
    "import re\n",
    "\n",
    "# Definindo as duplas de dados\n",
    "select_data = [[5, 20],[6, 44],[6, 88],[10, 100],[11, 11],[12, 3],[30, 12],[20,100],[24,6],[99,100],[96,16]]\n",
    "# Convertendo a lista de listas em um numpy array\n",
    "select_data = np.array(select_data)\n",
    "\n",
    "with open(\"dados_selecionados.txt\", \"w\") as f:\n",
    "  for i in wrong_predictions:\n",
    "    match = re.search(r'\\[([^\\]]+)\\]', i)\n",
    "    \n",
    "    if match:\n",
    "      extracted_text = (match.group(1)).split(', ')\n",
    "      int_data = np.array([int(num) for num in extracted_text])\n",
    "      is_present =  np.any(np.all(select_data == int_data, axis=1))\n",
    "      if is_present:\n",
    "        f.write(i + \" || \" + f\"MDC de {[int_data[1], int_data[0]]} - Previsto: {model.predict(np.array([[int_data[1], int_data[0]]]))[0]} - Real: {gcd(int_data[1], int_data[0])}\"+ \"\\n\")  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Obtém os pesos e bias da camada de Embedding\n",
    "embedding_weights = model.layers[0].get_weights()[0]\n",
    "\n",
    "# Obtém os pesos e bias da primeira camada LSTM\n",
    "lstm1_weights, lstm1_recurrent_weights, lstm1_bias = model.layers[1].get_weights()\n",
    "\n",
    "# Obtém os pesos e bias da segunda camada LSTM\n",
    "lstm2_weights, lstm2_recurrent_weights, lstm2_bias = model.layers[2].get_weights()\n",
    "\n",
    "# Obtém os pesos e bias da terceira camada LSTM\n",
    "lstm3_weights, lstm3_recurrent_weights, lstm3_bias = model.layers[3].get_weights()\n",
    "\n",
    "# Obtém os pesos e bias da camada densa\n",
    "dense_weights, dense_bias = model.layers[4].get_weights()\n",
    "\n",
    "# Função para plotar heatmaps em subplots\n",
    "def plot_weights_and_biases(weights, bias, title_weights, title_bias):\n",
    "  plt.figure(figsize=(14, 6))\n",
    "\n",
    "  # Plot weights\n",
    "  plt.subplot(1, 2, 1)\n",
    "  sns.heatmap(weights, cmap=\"viridis\", linewidths=.5)\n",
    "  plt.title(title_weights)\n",
    "\n",
    "  # Plot bias\n",
    "  plt.subplot(1, 2, 2)\n",
    "  sns.heatmap(bias.reshape(1, -1), cmap=\"viridis\", linewidths=.5)\n",
    "  plt.title(title_bias)\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "# Mapa de calor dos pesos e bias da camada de Embeddingc\n",
    "plot_weights_and_biases(embedding_weights, embedding_weights.mean(axis=0), 'Embedding Layer Weights', 'Embedding Layer Bias')\n",
    "\n",
    "# Mapa de calor dos pesos e bias da primeira camada LSTM\n",
    "plot_weights_and_biases(lstm1_weights, lstm1_bias, 'LSTM 1 Weights', 'LSTM 1 Bias')\n",
    "\n",
    "# Mapa de calor dos pesos e bias da segunda camada LSTM\n",
    "plot_weights_and_biases(lstm2_weights, lstm2_bias, 'LSTM 2 Weights', 'LSTM 2 Bias')\n",
    "\n",
    "# Mapa de calor dos pesos e bias da terceira camada LSTM\n",
    "plot_weights_and_biases(lstm3_weights, lstm3_bias, 'LSTM 3 Weights', 'LSTM 3 Bias')\n",
    "\n",
    "# Mapa de calor dos pesos e bias da camada densa\n",
    "plot_weights_and_biases(dense_weights, dense_bias, 'Dense Layer Weights', 'Dense Layer Bias')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
